{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# The Message Passing Interface MPI\n",
    "## Introduction  to MPI\n",
    "The Message Passing Interface (MPI) is:\n",
    "\n",
    "- Particularly useful for distributed memory machines\n",
    "- The _de facto_ standard parallel programming interface\n",
    "\n",
    "Many implementations exist - MPICH, OpenMPI, ...\n",
    "\n",
    "Interfaces in \n",
    "- C/C++ \n",
    "- Fortran and ... \n",
    "- Python wrappers (MPI4Py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Message Passing Paradigm\n",
    "The parallel program is launched as separate processes (tasks) each with their own address space.\n",
    "- To achieve parallelism we should partition data across tasks \n",
    "\n",
    "Data must be **explicitly moved** from process to process:\n",
    "- A task can access the data of another process through passing a message (a copy of the data is passed from one process to another)\n",
    "\n",
    "Two main classes of message passing:\n",
    "- **Point-to-point** operations, involving only two processes\n",
    "- **Collective** operations, involving a group of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI4Py\n",
    "MPI4Py provides an interface similar to the MPI standard C++ interface\n",
    "\n",
    "You can communicate Python objects\n",
    " - e.g. entire numpy arrays, rather than splitting into data and metadata that surrounds the object\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Communicators\n",
    "MPI uses communicator objects to identify a set of processes that can communicate with each other\n",
    "\n",
    "- `MPI_COMM_WORLD` is a default communicator, which contains all processes\n",
    "\n",
    "Processes have ranks \n",
    "- Unique process id in a given communicator, assigned by the system when the process initializes\n",
    "- Used to specify the sources and destinations of messages\n",
    "- 0 is the \"root process\", often used for I/O, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hello World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file hello_world_mpi.py\n",
    "#import the MPI class from mpi4py\n",
    "from mpi4py import MPI\n",
    "# call the COMM_WORLD attribute, store that in comm\n",
    "comm = MPI.COMM_WORLD\n",
    "# one of the attributes comm has is rank\n",
    "print(\"Hello world, I am process: \" + str(comm.rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mpirun -n 2 --mca mpi_cuda_support 0 python hello_world_mpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point to Point Communication\n",
    "\n",
    "Point-to-point communication is sending message/data from one process to another. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file send_and_receive.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "a = numpy.array([rank]*10, dtype=float)\n",
    "if rank == 0:\n",
    "    comm.send(a, dest = (rank + 1) % size)\n",
    "if rank > 0:\n",
    "    data = comm.recv(source = (rank - 1) % size)\n",
    "    comm.send(a, dest = (rank + 1) % size)\n",
    "if rank == 0:\n",
    "    data = comm.recv(source = size - 1)\n",
    "print(\"My ranks is \" + str(rank) + \"\\n and I received this array:\\n\" + str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mpirun -n 2 --mca mpi_cuda_support 0 python send_and_receive.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication \n",
    "Generally groups of processes need to exchange messages between themselves. Rather than explicitly sending and receiving such messages from point to point, MPI comes with group operations known as collectives.\n",
    "- Broadcast, scatter, gather and reduction\n",
    "- Implementations can optimize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Broadcast\n",
    "Send from one process to all other processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scatter\n",
    "Split data into chunks and send a chunk to individual processes to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gather\n",
    "Gather the chunks and bring them to the root process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduction\n",
    "Gather, and do some computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scatter\n",
    "We create an array on rank 0 and scatter it to all ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file scatter.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "sendbuf = []\n",
    "if comm.rank == 0:\n",
    "    m = numpy.random.randn(comm.size, comm.size)\n",
    "    print(\"Original array on root process\\n\" + str(m))\n",
    "    sendbuf = m\n",
    "# call this on every rank, including rank 0\n",
    "v = comm.scatter(sendbuf, root=0)\n",
    "print(\"I got this data:\\n\" + str(v) + \"\\n and my ranks is \" + str(comm.rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mpirun -n 2 --mca mpi_cuda_support 0 python scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gather\n",
    "Collect the results from all processes onto rank 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file gather.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "sendbuf = []\n",
    "\n",
    "if comm.rank == 0:\n",
    "    m = numpy.array(range(comm.size * comm.size), dtype=float)\n",
    "    m.shape=(comm.size, comm.size)\n",
    "    print(\"Original array on root process\\n\" + str(m))\n",
    "    sendbuf = m\n",
    "    \n",
    "# first scatter like before\n",
    "v = comm.scatter(sendbuf, root=0)\n",
    "print(\"I got this data:\\n\" + str(v) + \"\\n and my ranks is \" + str(comm.rank))\n",
    "#do some work on each process and then gather back onto root\n",
    "v = v * v\n",
    "recvbuf = comm.gather(v, root=0)\n",
    "if comm.rank == 0:\n",
    "        print(\"New array on rank 0:\\n\" + str(numpy.array(recvbuf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mpirun -n 2 --mca mpi_cuda_support 0 python gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reduction\n",
    "Create an array, scatter it, and do a parallel reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file reduce.py\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "comm = MPI.COMM_WORLD\n",
    "sendbuf = []\n",
    "\n",
    "if comm.rank == 0:\n",
    "    m = numpy.array(range(comm.size * comm.size), dtype=float)\n",
    "    m.shape=(comm.size, comm.size)\n",
    "    print(\"Original array on root process\\n\" + str(m))\n",
    "    sendbuf = m\n",
    "    \n",
    "# first scatter like before\n",
    "v = comm.scatter(sendbuf, root=0)\n",
    "print(\"I got this data:\\n\" + str(v) + \"\\n and my ranks is \" + str(comm.rank))\n",
    "\n",
    "recvbuf = comm.reduce(v, root=0)\n",
    "if comm.rank == 0:\n",
    "        print(\"New array on rank 0:\\n\" + str(numpy.array(recvbuf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mpirun -n 2 --mca mpi_cuda_support 0 python reduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%file scatter_uppercase.py\n",
    "<TO_DO>\n",
    "import numpy as np\n",
    "\n",
    "comm = <TO_DO>\n",
    "rank = <TO_DO>\n",
    "size = <TO_DO>\n",
    "\n",
    "A = np.zeros((size,size))\n",
    "if rank==0:\n",
    "    A = np.random.randn(size, size)\n",
    "    print(\"Original array on root process\\n\", A)\n",
    "local_a = np.zeros(size)\n",
    "\n",
    "comm.<TO_DO>(A, local_a, root=0)\n",
    "print(\"Process\", rank, \"received\", local_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mpirun -n 4 python scatter_uppercase.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
